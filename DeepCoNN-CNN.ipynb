{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# preprocessing imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions we implemented\n",
    "from custom_functions import init_embeddings_map, get_embed_and_pad_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 100\n",
    "embedding_map = init_embeddings_map(\"glove.6B.\" + str(emb_size) + \"d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"data/unembedded_grouped_cleaned_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split for our model is unique, we need to hold out a\n",
    "# set of users and movies so that our network never learns those \n",
    "test_size = 0.005\n",
    "\n",
    "# get test_size percentage of users\n",
    "unique_users = raw_data.loc[:, \"reviewerID\"].unique()\n",
    "users_size = len(unique_users)\n",
    "\n",
    "np.random.seed(2019)\n",
    "test_idx = np.random.choice(users_size,\n",
    "                              size=int(users_size * test_size),\n",
    "                              replace=False)\n",
    "\n",
    "# get test users\n",
    "test_users = unique_users[test_idx]\n",
    "\n",
    "# everyone else is a training user\n",
    "train_users = np.delete(unique_users, test_idx)\n",
    "\n",
    "test = raw_data[raw_data[\"reviewerID\"].isin(test_users)]\n",
    "train = raw_data[raw_data[\"reviewerID\"].isin(train_users)]\n",
    "\n",
    "unique_test_movies = test[\"asin\"].unique()\n",
    "\n",
    "# drop the movies that also appear in our test set. In order to be\n",
    "# a true train/test split, we are forced to discard some data entirely\n",
    "train = train.where(np.logical_not(train[\"asin\"].isin(unique_test_movies))).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_seq_sizes = raw_data.loc[:, \"userReviews\"].apply(lambda x: x.split()).apply(len)\n",
    "item_seq_sizes = raw_data.loc[:, \"movieReviews\"].apply(lambda x: x.split()).apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_ptile = 40\n",
    "i_ptile = 15\n",
    "u_seq_len = int(np.percentile(user_seq_sizes, u_ptile))\n",
    "i_seq_len = int(np.percentile(item_seq_sizes, i_ptile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_fn = get_embed_and_pad_func(i_seq_len, u_seq_len, np.array([0.0] * emb_size), embedding_map)\n",
    "    \n",
    "train_embedded = train.apply(embedding_fn, axis=1)\n",
    "test_embedded = test.apply(embedding_fn, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepCoNN Recommendation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeling imports\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers.merge import Add, Dot, Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepCoNN():\n",
    "    def __init__(self,\n",
    "                 embedding_size,\n",
    "                 hidden_size,\n",
    "                 u_seq_len,\n",
    "                 m_seq_len,\n",
    "                 filters=2,\n",
    "                 kernel_size=10):\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.inputU, self.towerU = self.create_deepconn_tower(u_seq_len)\n",
    "        self.inputM, self.towerM = self.create_deepconn_tower(m_seq_len)\n",
    "        self.joined = Concatenate()([self.towerU, self.towerM])\n",
    "        self.outNeuron = Dense(1)(self.joined)\n",
    "\n",
    "    def create_deepconn_tower(self, max_seq_len):\n",
    "        input_layer = Input(shape=(max_seq_len, self.embedding_size))\n",
    "        tower = Conv1D(filters=self.filters,\n",
    "                       kernel_size=self.kernel_size,\n",
    "                       activation=\"tanh\")(input_layer)\n",
    "        tower = MaxPooling1D()(tower)\n",
    "        tower = Flatten()(tower)\n",
    "        tower = Dense(self.hidden_size, activation=\"relu\")(tower)\n",
    "        return input_layer, tower\n",
    "\n",
    "    def create_deepconn_dp(self):\n",
    "        dotproduct = Dot(axes=1)([self.towerU, self.towerM])\n",
    "        output = Add()([self.outNeuron, dotproduct])\n",
    "        self.model = Model(inputs=[self.inputU, self.inputM], outputs=[output])\n",
    "        self.model.compile(optimizer='Adam', loss='mse')\n",
    "        \n",
    "    def train(self, train_data, batch_size, epochs=3500):\n",
    "        tensorboard = TensorBoard(log_dir=\"tf_logs/{}\".format(pd.Timestamp(int(time()), unit=\"s\")).replace(':', ''))\n",
    "        self.create_deepconn_dp()\n",
    "        print(self.model.summary())\n",
    "        \n",
    "        user_reviews = np.array(list(train_data.loc[:, \"userReviews\"]))\n",
    "        movie_reviews = np.array(list(train_data.loc[:, \"movieReviews\"]))\n",
    "\n",
    "        self.train_inputs = [user_reviews, movie_reviews]\n",
    "        self.train_outputs = train_data.loc[:, \"overall\"]\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=3, mode='min')\n",
    "        self.history = self.model.fit(self.train_inputs,\n",
    "                                      self.train_outputs,\n",
    "                                      callbacks=[tensorboard, early_stopping],\n",
    "                                      validation_split=0.05,\n",
    "                                      batch_size=batch_size,\n",
    "                                      epochs=epochs)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 243, 100)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 736, 100)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 234, 2)       2002        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 727, 2)       2002        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 117, 2)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 363, 2)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 234)          0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 726)          0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           15040       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           46528       flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128)          0           dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            129         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1)            0           dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 1)            0           dense_3[0][0]                    \n",
      "                                                                 dot_1[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 65,701\n",
      "Trainable params: 65,701\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 25840 samples, validate on 1360 samples\n",
      "Epoch 1/100\n",
      "25840/25840 [==============================] - 842s 33ms/step - loss: 1.2372 - val_loss: 1.8742\n",
      "Epoch 2/100\n",
      "25840/25840 [==============================] - 1147s 44ms/step - loss: 1.0493 - val_loss: 1.6611\n",
      "Epoch 3/100\n",
      "25840/25840 [==============================] - 1140s 44ms/step - loss: 0.9822 - val_loss: 1.5304\n",
      "Epoch 4/100\n",
      "25840/25840 [==============================] - 1013s 39ms/step - loss: 0.9524 - val_loss: 1.4531\n",
      "Epoch 5/100\n",
      "25840/25840 [==============================] - 1299s 50ms/step - loss: 0.9374 - val_loss: 1.4977\n",
      "Epoch 6/100\n",
      "25840/25840 [==============================] - 1358s 53ms/step - loss: 0.9054 - val_loss: 1.4444\n",
      "Epoch 7/100\n",
      "25840/25840 [==============================] - 1334s 52ms/step - loss: 0.8736 - val_loss: 1.4415\n",
      "Epoch 8/100\n",
      "25840/25840 [==============================] - 1218s 47ms/step - loss: 0.8556 - val_loss: 1.4755\n",
      "Epoch 9/100\n",
      "25840/25840 [==============================] - 1220s 47ms/step - loss: 0.8306 - val_loss: 1.5517\n",
      "Epoch 10/100\n",
      "25840/25840 [==============================] - 1157s 45ms/step - loss: 0.8178 - val_loss: 1.4775\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 64\n",
    "deepconn = DeepCoNN(emb_size, hidden_size, u_seq_len, i_seq_len)\n",
    "\n",
    "batch_size = 32\n",
    "deepconn.train(train_embedded, batch_size, epochs=100)\n",
    "\n",
    "deepconn.model.save(\"cnn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 3.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 3.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 3.]\n",
      " [ 4.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 3.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 3.]\n",
      " [ 4.]\n",
      " [ 3.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 3.]\n",
      " [ 4.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 1.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 3.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 3.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 4.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 2.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 1.]\n",
      " [ 3.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 3.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 3.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 4.]\n",
      " [ 3.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 1.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 2.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 3.]\n",
      " [ 4.]\n",
      " [ 2.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 4.]]\n",
      "[[ 4.56151772]\n",
      " [ 4.97018862]\n",
      " [ 4.31810474]\n",
      " [ 4.77266645]\n",
      " [ 4.45832109]\n",
      " [ 4.0411644 ]\n",
      " [ 3.88653064]\n",
      " [ 3.89498949]\n",
      " [ 4.65374994]\n",
      " [ 4.74176836]\n",
      " [ 4.76208305]\n",
      " [ 4.08102417]\n",
      " [ 4.08548212]\n",
      " [ 4.8026104 ]\n",
      " [ 4.18508101]\n",
      " [ 4.30528021]\n",
      " [ 4.42940044]\n",
      " [ 4.60958576]\n",
      " [ 4.38887453]\n",
      " [ 5.2537775 ]\n",
      " [ 3.96199155]\n",
      " [ 4.2477932 ]\n",
      " [ 4.16977549]\n",
      " [ 4.72938061]\n",
      " [ 4.07413721]\n",
      " [ 5.02970743]\n",
      " [ 4.39648199]\n",
      " [ 4.57611609]\n",
      " [ 3.49661326]\n",
      " [ 3.68154097]\n",
      " [ 3.4652133 ]\n",
      " [ 3.88699174]\n",
      " [ 4.66062069]\n",
      " [ 4.48112679]\n",
      " [ 4.10096455]\n",
      " [ 3.82927465]\n",
      " [ 4.70200491]\n",
      " [ 4.11778164]\n",
      " [ 4.00966692]\n",
      " [ 4.41799068]\n",
      " [ 4.10466337]\n",
      " [ 4.74497747]\n",
      " [ 4.43009615]\n",
      " [ 3.53819323]\n",
      " [ 4.62900925]\n",
      " [ 4.42627287]\n",
      " [ 4.27191448]\n",
      " [ 4.1995697 ]\n",
      " [ 4.22325087]\n",
      " [ 4.60055542]\n",
      " [ 3.08338118]\n",
      " [ 4.0372057 ]\n",
      " [ 4.31818676]\n",
      " [ 3.461169  ]\n",
      " [ 3.48749495]\n",
      " [ 4.09667683]\n",
      " [ 4.32462263]\n",
      " [ 4.33642483]\n",
      " [ 4.00746632]\n",
      " [ 3.07411838]\n",
      " [ 3.22998452]\n",
      " [ 4.08230162]\n",
      " [ 3.6390543 ]\n",
      " [ 4.27534676]\n",
      " [ 3.69030142]\n",
      " [ 4.05670691]\n",
      " [ 4.49574471]\n",
      " [ 3.72200584]\n",
      " [ 4.9307127 ]\n",
      " [ 4.1577878 ]\n",
      " [ 3.93876243]\n",
      " [ 3.83337784]\n",
      " [ 4.25505447]\n",
      " [ 3.39604068]\n",
      " [ 3.73580265]\n",
      " [ 4.58408451]\n",
      " [ 4.12443876]\n",
      " [ 3.761024  ]\n",
      " [ 4.22990799]\n",
      " [ 4.14737797]\n",
      " [ 4.8570652 ]\n",
      " [ 4.83786964]\n",
      " [ 4.36400795]\n",
      " [ 4.94771957]\n",
      " [ 3.79988456]\n",
      " [ 4.64346695]\n",
      " [ 3.76553035]\n",
      " [ 3.57489157]\n",
      " [ 4.33201456]\n",
      " [ 4.31330919]\n",
      " [ 3.63316941]\n",
      " [ 3.9001267 ]\n",
      " [ 4.16680098]\n",
      " [ 4.33400822]\n",
      " [ 4.17470264]\n",
      " [ 4.79546928]\n",
      " [ 4.33465719]\n",
      " [ 4.18598986]\n",
      " [ 4.05888414]\n",
      " [ 4.05916119]\n",
      " [ 4.43383312]\n",
      " [ 3.35822439]\n",
      " [ 4.52199793]\n",
      " [ 4.78755093]\n",
      " [ 3.91280055]\n",
      " [ 4.17723799]\n",
      " [ 4.65166759]\n",
      " [ 4.62530708]\n",
      " [ 4.45265007]\n",
      " [ 3.95964313]\n",
      " [ 4.69565678]\n",
      " [ 3.78268194]\n",
      " [ 4.36581469]\n",
      " [ 3.34622455]\n",
      " [ 3.89853835]\n",
      " [ 4.68692112]\n",
      " [ 4.29424524]\n",
      " [ 4.01400614]\n",
      " [ 3.8641355 ]\n",
      " [ 3.89182568]\n",
      " [ 4.40097713]\n",
      " [ 4.81462812]\n",
      " [ 4.4141264 ]\n",
      " [ 3.63839102]\n",
      " [ 3.73369527]\n",
      " [ 4.36190748]\n",
      " [ 4.51629925]\n",
      " [ 4.18688011]\n",
      " [ 4.46238565]\n",
      " [ 4.30386066]\n",
      " [ 3.24109888]\n",
      " [ 4.18522024]\n",
      " [ 5.23387909]\n",
      " [ 4.29152727]\n",
      " [ 4.20969248]\n",
      " [ 4.63979053]\n",
      " [ 4.33900356]\n",
      " [ 3.72583222]\n",
      " [ 4.76774931]\n",
      " [ 3.66544771]\n",
      " [ 3.99716878]\n",
      " [ 5.00846434]\n",
      " [ 3.88168931]\n",
      " [ 3.18486047]\n",
      " [ 4.12121248]\n",
      " [ 3.3765204 ]\n",
      " [ 2.86919761]\n",
      " [ 4.10393   ]\n",
      " [ 3.54504871]\n",
      " [ 3.0144546 ]\n",
      " [ 4.154953  ]\n",
      " [ 4.10490417]\n",
      " [ 3.05133939]\n",
      " [ 4.57292271]\n",
      " [ 4.26141167]\n",
      " [ 3.57062721]\n",
      " [ 4.09307241]\n",
      " [ 3.89525366]\n",
      " [ 3.61580634]\n",
      " [ 3.53994417]\n",
      " [ 4.58988762]\n",
      " [ 3.68872046]]\n",
      "MSE: 1.02864370802\n"
     ]
    }
   ],
   "source": [
    "user_reviews = np.array(list(test_embedded.loc[:, \"userReviews\"]))\n",
    "movie_reviews = np.array(list(test_embedded.loc[:, \"movieReviews\"]))\n",
    "\n",
    "test_inputs = [user_reviews, movie_reviews]\n",
    "\n",
    "dat = pd.DataFrame(test_inputs)\n",
    "dat.to_csv(\"data/test_data.csv\")\n",
    "\n",
    "true_rating = np.array(list(test_embedded.loc[:, \"overall\"])).reshape((-1, 1))\n",
    "print(true_rating)\n",
    "predictions = deepconn.model.predict(test_inputs)\n",
    "print(predictions)\n",
    "error = np.square(predictions - true_rating)\n",
    "\n",
    "print(\"MSE:\", np.average(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
