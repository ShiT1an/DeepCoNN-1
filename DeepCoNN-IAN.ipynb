{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# preprocessing imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions we implemented\n",
    "from custom_functions import init_embeddings_map, get_embed_and_pad_func, get_embed_aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 100\n",
    "embedding_map = init_embeddings_map(\"glove.6B.\" + str(emb_size) + \"d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_data = pd.read_csv(\"data/unembedded_grouped_cleaned_data.csv\")\n",
    "raw_data = pd.read_csv(\"data/automotive/unembedded_grouped_cleaned_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split for our model is unique, we need to hold out a\n",
    "# set of users and movies so that our network never learns those \n",
    "test_size = 0.005\n",
    "\n",
    "# get test_size percentage of users\n",
    "unique_users = raw_data.loc[:, \"reviewerID\"].unique()\n",
    "users_size = len(unique_users)\n",
    "\n",
    "np.random.seed(2019)\n",
    "test_idx = np.random.choice(users_size,\n",
    "                              size=int(users_size * test_size),\n",
    "                              replace=False)\n",
    "\n",
    "# get test users\n",
    "test_users = unique_users[test_idx]\n",
    "\n",
    "# everyone else is a training user\n",
    "train_users = np.delete(unique_users, test_idx)\n",
    "\n",
    "test = raw_data[raw_data[\"reviewerID\"].isin(test_users)]\n",
    "train = raw_data[raw_data[\"reviewerID\"].isin(train_users)]\n",
    "\n",
    "unique_test_movies = test[\"asin\"].unique()\n",
    "\n",
    "# drop the movies that also appear in our test set. In order to be\n",
    "# a true train/test split, we are forced to discard some data entirely\n",
    "train = train.where(np.logical_not(train[\"asin\"].isin(unique_test_movies))).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_seq_sizes = raw_data.loc[:, \"userReviews\"].apply(lambda x: x.split()).apply(len)\n",
    "item_seq_sizes = raw_data.loc[:, \"movieReviews\"].apply(lambda x: x.split()).apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_ptile = 40\n",
    "i_ptile = 15\n",
    "u_seq_len = int(np.percentile(user_seq_sizes, u_ptile))\n",
    "i_seq_len = int(np.percentile(item_seq_sizes, i_ptile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_fn = get_embed_and_pad_func(i_seq_len, u_seq_len, np.array([0.0] * emb_size), embedding_map)\n",
    "    \n",
    "train_embedded = train.apply(embedding_fn, axis=1)\n",
    "test_embedded = test.apply(embedding_fn, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeling imports\n",
    "from keras import regularizers\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Input, Dense, Permute, Reshape, RepeatVector, Activation, Lambda, GlobalAveragePooling1D, Dropout\n",
    "from keras.activations import tanh, softmax\n",
    "from keras.layers.merge import Add, Dot, Concatenate, Multiply\n",
    "from keras.backend import mean\n",
    "from MyLayer import AttentionScore, SelfAttentionScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IAN():\n",
    "    def __init__(self, embedding_size, hidden_size, rnn_hidden_size, u_seq_len, m_seq_len, filters=2, kernel_size=8,\n",
    "                 strides=6):\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.u_seq_len = u_seq_len\n",
    "        self.m_seq_len = m_seq_len\n",
    "        self.inputU, self.lstm_outU, self.mean_lstmoutU = self.create_deepconn_tower(self.u_seq_len)\n",
    "        self.inputM, self.lstm_outM, self.mean_lstmoutM = self.create_deepconn_tower(self.m_seq_len)\n",
    "    \n",
    "    def compute_attention_score(self, h, t, max_seq_len):\n",
    "        score = AttentionScore()([h, t])\n",
    "        alpha = Activation('softmax')(score)\n",
    "        tower = Dot(axes=1)([h, alpha])\n",
    "        return tower\n",
    "        \n",
    "    \n",
    "    def create_deepconn_tower(self, max_seq_len):\n",
    "        input_layer = Input(shape=(max_seq_len, self.embedding_size))\n",
    "        input_layer_drop = Dropout(0.5)(input_layer)\n",
    "        lstm_out = LSTM(self.rnn_hidden_size, activation=\"tanh\", return_sequences=True)(input_layer_drop)\n",
    "        # lstm_out.shape = (None, time_steps, input_dim)\n",
    "        print(lstm_out)\n",
    "        # mean_lstm_out = GlobalAveragePooling1D()(lstm_out)\n",
    "        self_attention_score = SelfAttentionScore()(lstm_out)\n",
    "        mean_lstm_out = Dot(axes=1)([lstm_out, self_attention_score])\n",
    "        print(mean_lstm_out.shape)\n",
    "        return input_layer, lstm_out, mean_lstm_out\n",
    "\n",
    "    def create_deepconn_dp(self):\n",
    "        towerU = self.compute_attention_score(self.lstm_outU, self.mean_lstmoutM, self.u_seq_len)\n",
    "        towerM = self.compute_attention_score(self.lstm_outM, self.mean_lstmoutU, self.m_seq_len)\n",
    "        output = Concatenate()([towerU, towerM])\n",
    "        output = Dense(1, activation='tanh', use_bias=True, kernel_regularizer=regularizers.l2(0.001))(output)\n",
    "        \n",
    "        dotproduct = Dot(axes=1)([towerU, towerM])\n",
    "        output = Add()([output, dotproduct])\n",
    "        # output = Activation('softmax')(output)\n",
    "        self.model = Model(inputs=[self.inputU, self.inputM], outputs=[output])\n",
    "        self.model.compile(optimizer='Adam', loss='mse')\n",
    "        \n",
    "    def train(self, train_data, batch_size, epochs=3500):\n",
    "        tensorboard = TensorBoard(log_dir=\"tf_logs/{}\".format(time()))\n",
    "        self.create_deepconn_dp()\n",
    "        print(self.model.summary())\n",
    "        \n",
    "        user_reviews = np.array(list(train_data.loc[:, \"userReviews\"]))\n",
    "        movie_reviews = np.array(list(train_data.loc[:, \"movieReviews\"]))\n",
    "       \n",
    "        self.train_inputs = [user_reviews, movie_reviews]\n",
    "        self.train_outputs = train_data.loc[:, \"overall\"]\n",
    "\n",
    "        # early_stopping = EarlyStopping(monitor='val_loss', patience=3, mode='min')\n",
    "        self.history = self.model.fit(self.train_inputs,\n",
    "                                      self.train_outputs,\n",
    "                                      callbacks=[tensorboard],\n",
    "                                      validation_split=0.05,\n",
    "                                      batch_size=batch_size,\n",
    "                                      epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"lstm_1/transpose_1:0\", shape=(?, ?, 64), dtype=float32)\n",
      "self attention input shape:\n",
      " (None, 318, 64)\n",
      "(?, 64)\n",
      "Tensor(\"lstm_2/transpose_1:0\", shape=(?, ?, 64), dtype=float32)\n",
      "self attention input shape:\n",
      " (None, 329, 64)\n",
      "(?, 64)\n",
      "AttentionScore input shape:\n",
      " [(None, 318, 64), (None, 64)]\n",
      "AttentionScore input shape:\n",
      " [(None, 329, 64), (None, 64)]\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 318, 100)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 329, 100)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 318, 100)     0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 329, 100)     0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 318, 64)      42240       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 329, 64)      42240       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "self_attention_score_2 (SelfAtt (None, 329)          393         lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "self_attention_score_1 (SelfAtt (None, 318)          382         lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 64)           0           lstm_2[0][0]                     \n",
      "                                                                 self_attention_score_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 64)           0           lstm_1[0][0]                     \n",
      "                                                                 self_attention_score_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "attention_score_1 (AttentionSco (None, 318)          4414        lstm_1[0][0]                     \n",
      "                                                                 dot_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "attention_score_2 (AttentionSco (None, 329)          4425        lstm_2[0][0]                     \n",
      "                                                                 dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 318)          0           attention_score_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 329)          0           attention_score_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dot_3 (Dot)                     (None, 64)           0           lstm_1[0][0]                     \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dot_4 (Dot)                     (None, 64)           0           lstm_2[0][0]                     \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128)          0           dot_3[0][0]                      \n",
      "                                                                 dot_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            129         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dot_5 (Dot)                     (None, 1)            0           dot_3[0][0]                      \n",
      "                                                                 dot_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 1)            0           dense_1[0][0]                    \n",
      "                                                                 dot_5[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 94,223\n",
      "Trainable params: 94,223\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 17081 samples, validate on 900 samples\n",
      "Epoch 1/20\n",
      "17081/17081 [==============================] - 334s 20ms/step - loss: 1.1709 - val_loss: 0.9370\n",
      "Epoch 2/20\n",
      "17081/17081 [==============================] - 322s 19ms/step - loss: 0.8971 - val_loss: 0.8852\n",
      "Epoch 3/20\n",
      "17081/17081 [==============================] - 324s 19ms/step - loss: 0.8932 - val_loss: 0.8883\n",
      "Epoch 4/20\n",
      "17081/17081 [==============================] - 322s 19ms/step - loss: 0.8862 - val_loss: 0.9143\n",
      "Epoch 5/20\n",
      "17081/17081 [==============================] - 324s 19ms/step - loss: 0.8859 - val_loss: 0.9597\n",
      "Epoch 6/20\n",
      "17081/17081 [==============================] - 322s 19ms/step - loss: 0.8861 - val_loss: 0.8778\n",
      "Epoch 7/20\n",
      "17081/17081 [==============================] - 323s 19ms/step - loss: 0.8790 - val_loss: 0.8797\n",
      "Epoch 8/20\n",
      "17081/17081 [==============================] - 324s 19ms/step - loss: 0.8785 - val_loss: 0.9542\n",
      "Epoch 9/20\n",
      "17081/17081 [==============================] - 322s 19ms/step - loss: 0.8760 - val_loss: 0.9073\n",
      "Epoch 10/20\n",
      "17081/17081 [==============================] - 322s 19ms/step - loss: 0.8718 - val_loss: 0.8871\n",
      "Epoch 11/20\n",
      "17081/17081 [==============================] - 322s 19ms/step - loss: 0.8690 - val_loss: 0.8723\n",
      "Epoch 12/20\n",
      "17081/17081 [==============================] - 321s 19ms/step - loss: 0.8682 - val_loss: 0.8866\n",
      "Epoch 13/20\n",
      "17081/17081 [==============================] - 322s 19ms/step - loss: 0.8627 - val_loss: 0.8998\n",
      "Epoch 14/20\n",
      "17081/17081 [==============================] - 329s 19ms/step - loss: 0.8593 - val_loss: 0.8740\n",
      "Epoch 15/20\n",
      "17081/17081 [==============================] - 334s 20ms/step - loss: 0.8558 - val_loss: 0.8941\n",
      "Epoch 16/20\n",
      "17081/17081 [==============================] - 336s 20ms/step - loss: 0.8502 - val_loss: 0.8940\n",
      "Epoch 17/20\n",
      "17081/17081 [==============================] - 328s 19ms/step - loss: 0.8466 - val_loss: 1.0016\n",
      "Epoch 18/20\n",
      "17081/17081 [==============================] - 327s 19ms/step - loss: 0.8445 - val_loss: 0.9150\n",
      "Epoch 19/20\n",
      "17081/17081 [==============================] - 328s 19ms/step - loss: 0.8350 - val_loss: 0.9097\n",
      "Epoch 20/20\n",
      "17081/17081 [==============================] - 322s 19ms/step - loss: 0.8376 - val_loss: 0.8802\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 64\n",
    "rnn_hidden_size = 64\n",
    "ian = IAN(emb_size, hidden_size, rnn_hidden_size, u_seq_len, i_seq_len)\n",
    "\n",
    "batch_size = 32\n",
    "ian.train(train_embedded, batch_size, epochs=20)\n",
    "\n",
    "ian.model.save(\"ian.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 3.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 2.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 1.]\n",
      " [ 4.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 4.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 3.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 3.]\n",
      " [ 4.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 3.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 1.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 2.]\n",
      " [ 5.]\n",
      " [ 1.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 1.]\n",
      " [ 5.]\n",
      " [ 3.]\n",
      " [ 5.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 4.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 4.]] [[ 4.70479774]\n",
      " [ 4.22012472]\n",
      " [ 4.65499353]\n",
      " [ 4.66837883]\n",
      " [ 4.57040596]\n",
      " [ 4.58980322]\n",
      " [ 4.38133478]\n",
      " [ 4.24962568]\n",
      " [ 4.56519938]\n",
      " [ 4.56311131]\n",
      " [ 4.60039186]\n",
      " [ 4.93430901]\n",
      " [ 4.46707916]\n",
      " [ 4.82047558]\n",
      " [ 4.86564636]\n",
      " [ 4.64112329]\n",
      " [ 4.72292042]\n",
      " [ 4.45028687]\n",
      " [ 4.59775925]\n",
      " [ 4.64789677]\n",
      " [ 4.90065813]\n",
      " [ 4.57088947]\n",
      " [ 4.3591733 ]\n",
      " [ 4.73140764]\n",
      " [ 4.76510191]\n",
      " [ 4.68644714]\n",
      " [ 4.35710573]\n",
      " [ 4.93425179]\n",
      " [ 4.50247192]\n",
      " [ 4.52752209]\n",
      " [ 4.83273649]\n",
      " [ 4.44072056]\n",
      " [ 4.5296979 ]\n",
      " [ 4.56215048]\n",
      " [ 5.01167583]\n",
      " [ 4.64234066]\n",
      " [ 4.80050802]\n",
      " [ 4.65765858]\n",
      " [ 4.54996109]\n",
      " [ 4.87908649]\n",
      " [ 4.62387323]\n",
      " [ 4.19623852]\n",
      " [ 4.60921955]\n",
      " [ 4.12064934]\n",
      " [ 4.57918596]\n",
      " [ 4.34142447]\n",
      " [ 4.52088594]\n",
      " [ 4.79516506]\n",
      " [ 4.59018612]\n",
      " [ 4.58673477]\n",
      " [ 4.57786417]\n",
      " [ 4.7440114 ]\n",
      " [ 4.44325113]\n",
      " [ 4.97153854]\n",
      " [ 4.83260489]\n",
      " [ 4.73044062]\n",
      " [ 4.84729576]\n",
      " [ 4.54898548]\n",
      " [ 4.55418873]\n",
      " [ 4.58783722]\n",
      " [ 4.47666264]\n",
      " [ 4.68479204]\n",
      " [ 4.81166601]\n",
      " [ 4.67745161]\n",
      " [ 4.84455776]\n",
      " [ 4.55530214]\n",
      " [ 4.66760159]\n",
      " [ 4.7090106 ]\n",
      " [ 4.39777136]\n",
      " [ 4.64331579]\n",
      " [ 4.29053783]\n",
      " [ 4.67473316]\n",
      " [ 4.70844984]\n",
      " [ 4.90363932]\n",
      " [ 4.58964062]\n",
      " [ 4.49755573]\n",
      " [ 4.46447563]\n",
      " [ 4.77737951]\n",
      " [ 4.37561655]\n",
      " [ 4.53097725]\n",
      " [ 4.40163517]\n",
      " [ 4.66357899]\n",
      " [ 4.41003466]\n",
      " [ 4.72603798]\n",
      " [ 4.81653786]\n",
      " [ 4.70842505]\n",
      " [ 4.21428108]\n",
      " [ 4.59955025]\n",
      " [ 4.64128017]\n",
      " [ 4.54234505]\n",
      " [ 4.50092888]\n",
      " [ 4.59897804]\n",
      " [ 4.62015009]\n",
      " [ 4.73284483]\n",
      " [ 4.50479078]\n",
      " [ 4.61138582]\n",
      " [ 4.37109375]\n",
      " [ 4.10723734]\n",
      " [ 4.25595427]\n",
      " [ 4.65692282]\n",
      " [ 4.502244  ]\n",
      " [ 4.46653223]\n",
      " [ 4.63058233]\n",
      " [ 4.70496702]\n",
      " [ 3.93556499]\n",
      " [ 4.11253309]\n",
      " [ 4.52436638]\n",
      " [ 4.66562557]\n",
      " [ 4.63366747]\n",
      " [ 4.80158615]\n",
      " [ 4.49959612]\n",
      " [ 4.27207232]\n",
      " [ 4.02176571]\n",
      " [ 4.56660271]\n",
      " [ 4.60093737]\n",
      " [ 4.3001523 ]\n",
      " [ 4.42883492]\n",
      " [ 4.61127329]\n",
      " [ 4.69079351]\n",
      " [ 4.46928501]\n",
      " [ 4.56712246]\n",
      " [ 4.66409969]\n",
      " [ 4.61244869]\n",
      " [ 4.62462997]\n",
      " [ 4.4922514 ]\n",
      " [ 4.57235813]\n",
      " [ 4.76242065]\n",
      " [ 4.25779772]\n",
      " [ 4.46375799]\n",
      " [ 4.38465977]\n",
      " [ 4.71685028]\n",
      " [ 4.63963509]\n",
      " [ 4.70716524]\n",
      " [ 4.57767725]\n",
      " [ 4.46368074]\n",
      " [ 4.78214741]\n",
      " [ 4.23333025]\n",
      " [ 4.44330168]]\n",
      "MSE: 0.910168061356\n"
     ]
    }
   ],
   "source": [
    "user_reviews = np.array(list(test_embedded.loc[:, \"userReviews\"]))\n",
    "movie_reviews = np.array(list(test_embedded.loc[:, \"movieReviews\"]))\n",
    "\n",
    "test_inputs = [user_reviews, movie_reviews]\n",
    "\n",
    "dat = pd.DataFrame(test_inputs)\n",
    "dat.to_csv(\"data/test_data.csv\")\n",
    "\n",
    "true_rating = np.array(list(test_embedded.loc[:, \"overall\"])).reshape((-1, 1))\n",
    "\n",
    "predictions = ian.model.predict(test_inputs)\n",
    "\n",
    "print(true_rating, predictions)\n",
    "error = np.square(predictions - true_rating)\n",
    "\n",
    "print(\"MSE:\", np.average(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#检查对于softmax的使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
